{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b334d371",
   "metadata": {},
   "source": [
    "## 1. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bec6f-6fe2-42e6-93fa-091ea1fb9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import apache_beam as beam\n",
    "from datetime import datetime\n",
    "from apache_beam import DoFn\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\n",
    "from apache_beam import DoFn, GroupByKey, io, ParDo, Pipeline, PTransform, WindowInto, WithKeys,Create,Map , CombineGlobally ,dataframe\n",
    "from apache_beam import CombineGlobally, CombinePerKey\n",
    "from apache_beam.runners.interactive import interactive_runner\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e6dc63-fc9f-4d4a-a030-6e610ab4c1a6",
   "metadata": {},
   "source": [
    "## 2. Create function to add window start and end to the aggregated windowed data\n",
    "\n",
    "This function will take transformed aggregated result and adds window start and end dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ef5a5d7f-8245-4203-8afb-b23e6dd359ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Window Starttime and Endtime to PCollections after the window aggregation\n",
    "class FormatDoFn(beam.DoFn):\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "        ts_format = '%Y-%m-%d %H:%M:%S.%f UTC'\n",
    "        window_start = datetime.fromtimestamp(window.start)\n",
    "        window_end = datetime.fromtimestamp(window.end)\n",
    "        return [{\n",
    "        'sensorID': element[0],\n",
    "        'sensorValue': element[1],\n",
    "        'windowStart': window_start,\n",
    "        'windowEnd': window_end\n",
    "        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80b2c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Window Starttime and Endtime to PCollections after the window aggregation\n",
    "class ProcessDoFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        yield element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f77bd4",
   "metadata": {},
   "source": [
    "## 3. Create function to add window start and end to the aggregated windowed data using Dataflow Runner\n",
    "Run this code only if you want to run this lab as a dataflow job. THIS WILL CREATE A DATAFLOW JOB. \n",
    "This function will write both raw data and aggregated data into two different BigQuery Tables. \n",
    "The below Beam Pipeline, aggregates the data using window functions to create Fixed 10second windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1171caad-b0c7-488b-959a-6a37f8373eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(runneroption):\n",
    "    # Parsing arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--input_subscription\",\n",
    "        help='Input PubSub subscription of the form \"projects/<PROJECT>/subscriptions/<SUBSCRIPTION>.\"',\n",
    "        default=PUBSUB_SUBSCRIPTION,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_raw_table\", help=\"Output BigQuery Table\", default=BIGQUERY_RAW_TABLE\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_agg_table\", help=\"Output BigQuery Table\", default=BIGQUERY_AGG_TABLE\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_raw_schema\",\n",
    "        help=\"Output BigQuery Schema in text format\",\n",
    "        default=BIGQUERY_RAW_SCHEMA,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_schema\",\n",
    "        help=\"Output BigQuery Schema in text format\",\n",
    "        default=BIGQUERY_SCHEMA,\n",
    "    )\n",
    "    known_args, pipeline_args = parser.parse_known_args()\n",
    "\n",
    "    \n",
    "    if runneroption == 'DataflowRunner':\n",
    "        pipeline_options = PipelineOptions(\n",
    "        pipeline_args,\n",
    "        runner=runneroption,\n",
    "        project=DEST_PROJECT,\n",
    "        job_name='unique-job-name',\n",
    "        temp_location=BUCKET_NAME,\n",
    "        region=REGION,\n",
    "        service_account_email=SERVICE_ACCOUNT_FQN,\n",
    "        network=VPC_FQN,\n",
    "        subnetwork=SUBNET)\n",
    "    else:\n",
    "        pipeline_options = PipelineOptions(\n",
    "        pipeline_args,\n",
    "        runner=runneroption)\n",
    "    pipeline_options.view_as(StandardOptions).streaming = True    \n",
    "    with beam.Pipeline(options=pipeline_options) as p:\n",
    "            mapped =     (\n",
    "                \n",
    "                p\n",
    "                  | \"ReadFromPubSub\" >> beam.io.gcp.pubsub.ReadFromPubSub(subscription=PUBSUB_SUBSCRIPTION)\n",
    "                  | \"Json Loads\" >> Map(json.loads))\n",
    "            raw_data = (\n",
    "                mapped             \n",
    "                        \n",
    "                  | 'Format' >> beam.ParDo(ProcessDoFn())\n",
    "                  |  \"WriteToBigQueryRaw\" >> beam.io.WriteToBigQuery(\n",
    "                known_args.output_raw_table,\n",
    "                schema=known_args.output_raw_schema,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND))\n",
    "            agg_data = (mapped\n",
    "                        \n",
    "                  | \"Map Keys\" >> Map(lambda x: (x[\"SensorID\"],x[\"SensorValue\"]))\n",
    "                  | \"ApplyFixedWindow\" >> beam.WindowInto(beam.window.FixedWindows(3))\n",
    "                  | \"Total Per Key\" >> beam.combiners.Mean.PerKey()\n",
    "                  | 'Final Format' >> beam.ParDo(FormatDoFn())\n",
    "                  |  \"WriteToBigQuery\" >> beam.io.WriteToBigQuery(\n",
    "                known_args.output_agg_table,\n",
    "                schema=known_args.output_schema,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d622802-9974-4674-91a7-17667fd21aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run(runneroption =\"dataflowrunner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT sensorValue,row_number() over (order by windowStart)  as cycle FROM `general-demo-364117.Asset_Management_Demo.Anomaly-detection-dataflow` limit 500;\n",
    "\"\"\"\n",
    "pct_overlap_terms_by_days_apart = client.query(sql).to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef127fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_overlap_terms_by_days_apart.plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"cycle\",\n",
    "    y=\"sensorValue\"  , color = 'red',\n",
    "    figsize=(20, 10)\n",
    "        \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01. Apache Beam 2.45.0 for Python 3",
   "language": "python",
   "name": "01-apache-beam-2.45.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
